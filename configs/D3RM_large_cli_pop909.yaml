seed_everything: 23
# ckpt_path : checkpoints/2024-12-31T01-41-17/step=0000050-metric_note_with_offsets_f1=0.0034.ckpt
wandb: True
debug: False

trainer:
  benchmark: True
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  devices: 1 
  accelerator: gpu
  strategy: ddp
  log_every_n_steps: 10
  check_val_every_n_epoch: null
  val_check_interval: 2500
  # enable_checkpointing: True
  max_steps: 800000

  callbacks:
    - class_path: transcription.callbacks.ema.EMACallback
      init_args:
        decay: 0.99
        use_ema_weights: True
        update_interval: 25
    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #     monitor: metric_note_with_offsets_f1
    #     mode: max
    #     save_top_k: 5
    #     save_last: True
    #     filename: '{step:07}-{metric_note_with_offsets_f1:.4f}'
    #     dirpath: './checkpoints/'
    #     verbose: True

model:
  encoder_parameters: no_pretrain
  pretrained_encoder_path: model_170k_0.9063_nonar.pt
  freeze_encoder: False
  test_save_path: './results/'
  # use_ema: True
  # ema_decay: 0.99
  # ema_update_interval: 25

  label_seq_len: 11264 # to 128 * 88
  diffusion_step: 100
  gamma_bar_T: 0.6
  auxiliary_loss_weight: 5.0e-4
  adaptive_auxiliary_loss: true
  mask_weight: [1, 1]
  classifier_free_guidance: False
  onset_suppress_sample: False
  onset_weight_kl: False
  no_mask: False
  sample_from_fully_masked: True
  reverse_sampling: True
  inpainting_ratio: 0 # how much to inpaint
  repaint: 5
  use_style_enc: True
  style_enc_ckpt: pretrained/polydis/model_master_final.pt

  encoder:
    class_path: transcription.decoder.LSTM_NATTEN
    init_args:
      hidden: 4
      timestep_type: null
      diffusion_step: 100
      natten_direction: 2d
      dilation: [1,2,4,8,16,1,2,4,8,16]
      spatial_size: [128,88]
      window: [5,5,5,5,5,5,5,5,5,5]
      n_unit: 96
      n_layers: 10
      cross_condition: self
      type: encoder

  decoder:  
    class_path: transcription.decoder.TransModel
    init_args:
      label_embed_dim: 4
      lstm_dim: 96
      n_layers: 10
      window: [5,5,5,5,5,5,5,5,5,5]
      dilation: [1,2,4,8,16,1,2,4,8,16]
      condition_method: self
      diffusion_step: 100
      timestep_type: adalayernorm
      natten_direction: 2d
      spatial_size: [128,88]
      num_state: 4
      num_class: 3
      classifier_free_guidance: False
      features_embed_dim: 96

  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1.0e-3
      weight_decay: 4.5e-2
      betas: [0.9, 0.96]

  scheduler:
    monitor: train/diffusion_loss
    factor: 0.8
    patience: 25000
    min_lr: 1.0e-5
    threshold: 1.0e-1
    threshold_mode: rel
    # warmup_lr: 4.5e-4
  # warmup: 1000


data:
  data_dir: './data/POP909-Dataset/POP909_processed'
  batch_size: 8 
  num_workers: 12
  # train: True
  # validation: True
  # test: True
  # predict: False
  train_seq_len: 128
  valid_seq_len: 128
  # train_seq_len: 160256
  # valid_seq_len: 160256
  # sampling_rate: 16000
  hop_size: 512
  transpose: true
  pr_res: 16

  